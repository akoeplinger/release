#!/usr/bin/env python
#
# Usage: build os-target package version [serial]
#
# Where:
#    os-target is the OS target that we are building for
#    package is the name of the BB XML configuration file on the repository
#    version is the version you want to build
#    serial is optionally the build number. If it is not present, it is assumed
#      to be zero.
#
# Example:
#    build suse-92-i386 mono 1.1.6
#

#TODO catch SIGQUIT and update status to 'killed'

import sys
import glob
import os.path
import os
import distutils.dir_util
import signal
import getopt

sys.path += [ '../pyutils' ]
import packaging
import logger
import utils
import datastore
import config

import pdb

timeout=1200
# Set max output to 10 MB, so the remote process dies if it exceed this. 
#  (prevents us with ending up with 1GB log files.  10 MB should be sufficient)
max_output_size = 10 * 1024 * 1024

def get_state(code, test=0):
	print "Return code: %d" % code
	#  214 is for killed subprocess
	#  137 is for killed process groups
	#  255 seems to be returned through ssh on a killed process...
	#  How accurrate are these exit codes??
	#  Some tests are reporting 255 on killed processes?
	if code == utils.KILLED_EXIT_CODE or code == 214 or code == 137 or code == 255:
		return "timeout"
	elif code:
		if test:
			return "testfailure"
		else:
			return "failure"
	else:
		return "success"

def run_postbuild_step(name, step_key, test=0):

	command = package.get_info_var(step_key)

	code = 0

	if not (name and command):
		print "run_buildbuild_step: Invalid params: '%s', '%s'" % (name, step_key)
	else:
		print "Running %s step!" % name
		LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', '%s.log' % name)
		log_obj = logger.Logger(filename=LOGFILE, print_screen=print_screen)
		build_info.update_step(name, state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")

		(code, output) = build_env.ssh.execute("%s cd %s  " % (step_env_pre_cmd, remote_src_dir + command), env=build_env.env_vars, logger=log_obj, output_timeout=timeout, max_output_size=max_output_size)
		state = get_state(code, test=test)
		if code:
			print "Step %s failed..." % name
			build_info.update_step(name, state=state, finish=utils.get_time())
		else:
			build_info.update_step(name, state=state, finish=utils.get_time())

	return code


# Collect optional opts
skip_steps = False
# This is to specify installing release packages even when building a package from head
install_release_packages = False
# Don't print to the screen
print_screen = 1
opts, remaining_args = getopt.getopt(sys.argv[1:], "", [ "skip_steps", "install_release_packages", "suppress_output" ])
for option, value in opts:
	if option == "--skip_steps":
		 skip_steps = True
	if option == "--install_release_packages":
		 install_release_packages = True
	if option == "--suppress_output":
		 print_screen = 0

if len(remaining_args) < 3:
	print "Usage is: ./build [--skip_steps] [--install_release_packages] [--suppress_output] configuration package version <release>"
        print "  <release> is optional"
	sys.exit(1)

distro = remaining_args[0]
package_name = remaining_args[1]
version = remaining_args[2]

# If serial is passed in
if len(remaining_args) > 3:
	serial = remaining_args[3]
else:	serial = "0"


# Figure out if this is a snapshot build or not...
#  Could do this by looking at the tarball map... ? (but ./build parameters would have to change)

if not os.path.exists("defs/" + package_name):
        print "Error, file not found: defs/" + package_name
	sys.exit(1)

sources = []
snapshot_sources = []

for ext in config.source_extensions:
	sources += glob.glob("sources/%s/*-%s%s" % (package_name, version, ext))

for ext in config.source_extensions:
	snapshot_sources += glob.glob("snapshot_sources/%s/*-%s%s" % (package_name, version, ext))

if sources:
	print "Using release sources and packages"
	HEAD_or_RELEASE = "RELEASE"
elif snapshot_sources:
	print "Using snapshot sources and packages"
	HEAD_or_RELEASE = "HEAD"
else:
	print "Cannot find source file for " + package_name
	sys.exit(4)

sources += snapshot_sources
source_file = sources[0]

build_env = packaging.buildenv(distro)
package = packaging.package(build_env, package_name, HEAD_or_RELEASE=HEAD_or_RELEASE)

# Hack to for some numbering schemes (nant, ironpython) to be rpm version sorting friendly
if package.get_info_var('APPEND_ZERO_TO_RPM_VERSION'):
	version += '.0'

if serial == "0":
	ver_path = version
else:
	ver_path = version + "-" + serial

# check to see if this package is a valid build
if not package.valid_build_platform(distro):
	print "%s is not in BUILD_HOSTS" % distro
	sys.exit(3)


if build_env.is_locked():
	print "%s jail is busy" % distro
	sys.exit(2)

build_env.lock_env()

# Set signal handler
def keyboard_interrupt(signum, frame):
	print 'Build aborted:', signum
	build_env.unlock_env()
signal.signal(signal.SIGINT, keyboard_interrupt)

if build_env.offline():
	print "%s jail is offline" % distro
	build_env.unlock_env()
	sys.exit(2)


revision = package.get_revision(serial)

package_path = package.package_fullpath

print package_path

package_version_path = package_path + os.sep + ver_path

if os.path.exists(package_version_path):
	print "The path for this package (%s) already exists. You probably need to bump the revision number." % package_version_path
	build_env.unlock_env()
	sys.exit(5)

build_info = datastore.build_info(HEAD_or_RELEASE, distro, package_name, ver_path)
build_info.new_build()

# Update build step to running
build_info.update_build(state='inprogress', buildhost=build_env.info['target_host'], start=utils.get_time(), finish="")

# Debug
#values = build_info.get_build_info()
#pdb.set_trace()
#steps = build_info.get_steps_info()

# Update step: Add link to mktarball log
# Check for package aliases
repo = datastore.source_file_repo()
aliases = package.get_aliases()
logfile = repo.get_log_file(source_file, package_name_aliases=aliases)

if logfile:
	# Create symbolic link to tarball in 'files' dir
	# Create symbolic link to tarball_log in 'logs' dir
	# Make the symbolic links relative
	try:
		os.symlink("../../../../../../../../" + logfile, os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', os.path.basename(logfile) ) )
		os.symlink("../../../../../../../../packaging/" +source_file, os.path.join(config.build_info_dir, build_info.rel_files_dir, 'files', os.path.basename(source_file) ) )
	except OSError:
		# Usually means links are already there...
		pass

	build_info.update_step("mktarball", state='success', log=os.path.basename(logfile), download=os.path.basename(source_file))

# Update step: installing deps
LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', 'install-deps.log')
log_obj = logger.Logger(filename=LOGFILE, print_screen=print_screen)
build_info.update_step("install-deps", state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")

if install_release_packages:
	install_deps_H_or_R = "RELEASE"
	print "Overriding to use release packages..."
else:
	install_deps_H_or_R = HEAD_or_RELEASE
print "Installing dependencies..."
(code, output) = utils.launch_process("./install-deps --HEAD_or_RELEASE=%s %s %s" % (install_deps_H_or_R, distro, package_name), logger=log_obj)
if code:
	build_env.unlock_env()
	build_info.update_step("install-deps", state="failure", finish=utils.get_time())
	build_info.update_build(state="failure", finish=utils.get_time())
	print "Dependency installation failed, see log in %s for details" % LOGFILE
	sys.exit(6)

build_info.update_step("install-deps", state="success", finish=utils.get_time())

# Debug
#build_env.unlock_env()
#sys.exit(1)

# Update step: building
LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', 'build.log')
log_obj = logger.Logger(filename=LOGFILE, print_screen=print_screen)
build_info.update_step("build", state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")
# Copy tar.gz and zip files
files_to_copy = [ source_file ]

# Get the dir that tarball unpacks
# note: only works if resulting dir name is based off tarball name, and has one of the extions in config.source_extions
build_dir = os.path.basename(source_file)
for ext in config.source_extensions:
	build_dir = build_dir.replace(ext, '')

# Set optional custom build_location
build_location = build_env.env_vars['build_location']

# Files for zip build system and build command
if build_env.get_info_var('USE_ZIP_PKG'):
	# do-zip-build deps...
	files_to_copy += '../pyutils/config.py ../pyutils/utils.py ../pyutils/sshutils.py ../pyutils/packaging.py ../pyutils/shell_parse.py'.split()
	for f in aliases:
		files_to_copy += ['defs/' + f]
	files_to_copy += ['do-zip-build', 'conf/' + distro, '../conf/%s/*.patch' % package_name, 'defs/%s' % package_name ]
	build_command = '%s/do-zip-build %s %s %s %s %s' % (build_location, revision, package_name, distro, version, build_location)
	remote_src_dir = "%s/scratch/%s" % (build_location, build_dir)
	# This will never be at build_location, always at /tmp/build_deps so do-zip-install-pkgs will correctly relocate
	step_env_pre_cmd = ". /tmp/build_deps/env.sh; "

else:
	# Some 3rd party modules use '-' in their versions
	rpm_version = version.replace('-', '_')
	files_to_copy += ['do-build', '../conf/%s/*' % package_name ]
	build_command = '%s/do-build %s %s %s' % (build_location, rpm_version, revision, build_location)
	remote_src_dir = "%s/scratch/BUILD/%s" % (build_location, build_dir)
	step_env_pre_cmd = ""


build_env.ssh.copy_to(files_to_copy, build_location, mode='scp', compress=0)

# Set up env for the build and for tests
build_env.env_vars['HEAD_or_RELEASE'] = "HEAD"
build_env.env_vars['DISTRO'] = distro
build_env.env_vars['BUILD_OS'] = build_env.info['os']
build_env.env_vars['BUILD_ARCH'] = build_env.info['arch']

# Put in a function rather than copy and paste...
def build_failed(build_env, code, LOGFILE, build_info, package):
	build_env.unlock_env()
	state = get_state(code)
	print "Build failed, see log in %s for details" % LOGFILE
	build_info.update_step("build", state=state, finish=utils.get_time())
	if package.get_info_var('CLEAN_UP'):
		run_postbuild_step('clean-up', 'CLEAN_UP')
	build_info.update_build(state="failure", finish=utils.get_time())
	sys.exit(7)

(code, output) = build_env.ssh.execute(build_command, env=build_env.env_vars, logger=log_obj, output_timeout=timeout, max_output_size=max_output_size)
if code:
	build_failed(build_env, code, LOGFILE, build_info, package)

distutils.dir_util.mkpath(package_version_path)
build_env.ssh.copy_from(['%s/builder/built-packages/*' % build_location, '%s/scratch/*.spec' % build_location], package_version_path)

# Make sure packages got here ok
log_obj.log("Verifying packages...\n")

files = []
if build_env.get_info_var('USE_ZIP_PKG'):
	files = glob.glob(package_version_path + os.sep + "*.zip")
	code = 0
	output = ""
	for file in files:
		(tmp_code, tmp_output) = utils.launch_process("unzip -qt " + file, logger=log_obj)
		if tmp_code: code = 1
		output += tmp_output
else:
	files = glob.glob(package_version_path + os.sep + "*.rpm")
	(code, output) = utils.launch_process("rpm -K " + " ".join(files), logger=log_obj)

if code:
	log_obj.log("Verifying packages failed... (usually because of lack of disk space or incomplete copy)\n")

	# Seems sometimes rmtree doesn't full work...
	try:
		log_obj.log("Removing build...\n")
		for f in files:
			os.remove(f)
		os.rmdir(package_version_path)
		# Ugh, broken...
		#shutil.rmtree(package_version_path)
	except:
		#log_obj.log("Error removing build... (possibly a bug in python's shutil.rmtree)\n")
		#print "Error removing build... (bug in python)"
		log_obj.log("Error removing build...\n")
		print "Error removing build..."
	build_failed(build_env, code, LOGFILE, build_info, package)

log_obj.log("Packages verified!\n")

# Create link to packages
try:
	os.symlink(os.path.join("../../../../../../../../packaging/", package.package_base_relpath,  package.package_relpath,  ver_path), os.path.join(config.build_info_dir, build_info.rel_files_dir, 'files', 'downloads' ) )
except OSError:
	# Probably already exists...
	pass

build_info.update_step("build", state="success", download=os.path.basename('downloads'), finish=utils.get_time())

# Run postbuild steps
if not skip_steps:
	counter = 1
	while(1):
		name = package.get_info_var('POSTBUILD_STEP_NAME%d' % counter)
		step_key = 'POSTBUILD_STEP%d' % counter
		
		if name and package.info.has_key(step_key):
			run_postbuild_step(name, step_key, test=1)
		else:
			# We've run out of steps, stop
			break

		counter += 1

# Run cleanup step
if package.get_info_var('CLEAN_UP'):
	run_postbuild_step('clean-up', 'CLEAN_UP')

state = build_info.get_collective_state()
build_info.update_build(state=state, finish=utils.get_time())

build_env.unlock_env()

print "Done!"

