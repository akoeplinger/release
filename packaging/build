#!/usr/bin/env python
#
# Usage: build os-target package version [serial]
#
# Where:
#    os-target is the OS target that we are building for
#    package is the name of the BB XML configuration file on the repository
#    version is the version you want to build
#    serial is optionally the build number. If it is not present, it is assumed
#      to be zero.
#
# Example:
#    build suse-92-i386 mono 1.1.6
#

#TODO catch SIGQUIT and update status to 'killed'

import sys
import glob
import os.path
import os
import distutils.dir_util
import signal
import getopt

sys.path += [ '../pyutils' ]
import packaging
import logger
import utils
import datastore
import config

import pdb

timeout=1200

def get_state(code, test=0):
	print "Return code: %d" % code
	#  214 is for killed subprocess
	#  137 is for killed process groups
	#  255 seems to be returned through ssh on a killed process...
	#  How accurrate are these exit codes??
	#  Some tests are reporting 255 on killed processes?
	if code == utils.KILLED_EXIT_CODE or code == 214 or code == 137 or code == 255:
		return "timeout"
	elif code:
		if test:
			return "testfailure"
		else:
			return "failure"
	else:
		return "success"

# Collect optional opts
skip_steps = False
opts, remaining_args = getopt.getopt(sys.argv[1:], "", [ "skip_steps" ])
for option, value in opts:
	if option == "--skip_steps":
		 skip_steps = True

if len(remaining_args) < 3:
	print "Usage is: ./build [--skip_steps] configuration package version <release>"
        print "  <release> is optional"
	sys.exit(1)

distro = remaining_args[0]
package_name = remaining_args[1]
version = remaining_args[2]

# If serial is passed in
if len(remaining_args) > 3:
	serial = remaining_args[3]
else:	serial = "0"

if serial == "0":
	ver_path = version
else:
	ver_path = version + "-" + serial

# Figure out if this is a snapshot build or not...
#  Could do this by looking at the tarball map... ? (but ./build parameters would have to change)
sources = glob.glob("sources/%s/*-%s.tar.gz" % (package_name, version)) + glob.glob("sources/%s/*-%s.zip" % (package_name, version))
snapshot_sources = glob.glob("snapshot_sources/%s/*-%s.tar.gz" % (package_name, version)) + glob.glob("snapshot_sources/%s/*-%s.zip" % (package_name, version))
if sources:
	print "Using release sources and packages"
	HEAD_or_RELEASE = "RELEASE"
elif snapshot_sources:
	print "Using snapshot sources and packages"
	HEAD_or_RELEASE = "HEAD"
else:
	print "Cannot find source file for " + package_name
	sys.exit(4)

sources += snapshot_sources
source_file = sources[0]

build_env = packaging.buildenv(distro)
package = packaging.package(build_env, package_name, HEAD_or_RELEASE=HEAD_or_RELEASE)

# check to see if this package is a valid build
if not package.valid_build_platform(distro):
	print "%s is not in BUILD_HOSTS" % distro
	sys.exit(3)


if build_env.is_locked():
	print "%s jail is busy" % distro
	sys.exit(2)

build_env.lock_env()

# Set signal handler
def keyboard_interrupt(signum, frame):
	print 'Build aborted:', signum
	build_env.unlock_env()
signal.signal(signal.SIGINT, keyboard_interrupt)

if build_env.offline():
	print "%s jail is offline" % distro
	build_env.unlock_env()
	sys.exit(2)


revision = package.get_revision(serial)

package_path = package.package_fullpath

print package_path

package_version_path = package_path + os.sep + ver_path

if os.path.exists(package_version_path):
	print "The path for this package (%s) already exists. You probably need to bump the revision number" % package_version_path
	build_env.unlock_env()
	sys.exit(5)

build_info = datastore.build_info(HEAD_or_RELEASE, distro, package_name, ver_path)
build_info.new_build()

# Update build step to running
build_info.update_build(state='inprogress', buildhost=build_env.info['target_host'], start=utils.get_time(), finish="")

# Debug
#values = build_info.get_build_info()
#pdb.set_trace()
#steps = build_info.get_steps_info()

# Update step: Add link to mktarball log
# Check for package aliases
repo = datastore.source_file_repo()
aliases = package.get_aliases()
logfile = repo.get_log_file(source_file, package_name_aliases=aliases)

if logfile:
	# Create symbolic link to tarball in 'files' dir
	# Create symbolic link to tarball_log in 'logs' dir
	# Make the symbolic links relative
	try:
		os.symlink("../../../../../../../../" + logfile, os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', os.path.basename(logfile) ) )
		os.symlink("../../../../../../../../packaging/" +source_file, os.path.join(config.build_info_dir, build_info.rel_files_dir, 'files', os.path.basename(source_file) ) )
	except OSError:
		# Usually means links are already there...
		pass

	build_info.update_step("mktarball", state='success', log=os.path.basename(logfile), download=os.path.basename(source_file))

# Update step: installing deps
LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', 'install-deps.log')
log_obj = logger.Logger(filename=LOGFILE, print_screen=0)
build_info.update_step("install-deps", state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")

print "Installing dependencies..."
(code, output) = utils.launch_process("./install-deps %s %s" % (distro, package_name), logger=log_obj)
if code:
	build_env.unlock_env()
	build_info.update_step("install-deps", state="failure", finish=utils.get_time())
	build_info.update_build(state="failure", finish=utils.get_time())
	print "Dependency installation failed, see log in %s for details" % LOGFILE
	sys.exit(6)

build_info.update_step("install-deps", state="success", finish=utils.get_time())

# Debug
#build_env.unlock_env()
#sys.exit(1)

# Update step: building
LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', 'build.log')
log_obj = logger.Logger(filename=LOGFILE, print_screen=0)
build_info.update_step("build", state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")
# Copy tar.gz and zip files
files_to_copy = [ source_file ]

# Get the dir that tarball unpacks
#   Will this work with all tarballs?  This is what we do for do-zip-build...
build_dir = os.path.basename(source_file).replace('.tar.gz', '')
build_dir = build_dir.replace('.zip', '')

# Set optional custom build_location
if build_env.info.has_key('build_location'):
	build_location = build_env.info['build_location']
else:
	build_location = "/tmp"

# Files for zip build system and build command
if build_env.get_info_var('USE_ZIP_PKG'):
	# do-zip-build deps...
	files_to_copy += '../pyutils/config.py ../pyutils/utils.py ../pyutils/sshutils.py ../pyutils/packaging.py ../pyutils/shell_parse.py'.split()
	for f in aliases:
		files_to_copy += ['defs/' + f]
	files_to_copy += ['do-zip-build', 'conf/' + distro, '../conf/%s/*.patch' % package_name, 'defs/%s' % package_name ]
	build_command = '%s/do-zip-build %s %s %s %s' % (build_location, revision, package_name, distro, version)
	remote_src_dir = "%s/scratch/%s" % (build_location, build_dir)
	step_env_pre_cmd = ". %s/build_deps/env.sh; " % build_location

else:
	files_to_copy += ['do-build', '../conf/%s/*' % package_name ]
	build_command = '%s/do-build %s %s %s' % (build_location, version, revision, build_location)
	remote_src_dir = "%s/scratch/BUILD/%s" % (build_location, build_dir)
	step_env_pre_cmd = ""


build_env.ssh.copy_to(files_to_copy, build_location, mode='scp')

(code, output) = build_env.ssh.execute(build_command, logger=log_obj, output_timeout=timeout)
#code = 0
if code:
	build_env.unlock_env()
	state = get_state(code)
	print "Build failed, see log in %s for details" % LOGFILE
	build_info.update_step("build", state=state, finish=utils.get_time())
	build_info.update_build(state="failure", finish=utils.get_time())
	sys.exit(7)

distutils.dir_util.mkpath(package_version_path)
build_env.ssh.copy_from(['%s/builder/built-packages/*' % build_location, '%s/scratch/*.spec' % build_location], package_version_path)

# Create link to packages
try:
	os.symlink(os.path.join("../../../../../../../../packaging/", package.package_base_relpath,  package.package_relpath,  ver_path), os.path.join(config.build_info_dir, build_info.rel_files_dir, 'files', 'downloads' ) )
except OSError:
	# Probably already exists...
	pass

build_info.update_step("build", state="success", download=os.path.basename('downloads'), finish=utils.get_time())

# Run postbuild steps
if not skip_steps:
	remote_env = {}
	remote_env['HEAD_or_RELEASE'] = "HEAD"
	counter = 1
	while(1):
		if package.info.has_key('POSTBUILD_STEP_NAME%d' % counter) and package.info.has_key('POSTBUILD_STEP%d' % counter):
			print "Running step %d!" % counter
			name = package.info['POSTBUILD_STEP_NAME%d' % counter]
			LOGFILE = os.path.join(config.build_info_dir, build_info.rel_files_dir, 'logs', '%s.log' % name)
			log_obj = logger.Logger(filename=LOGFILE, print_screen=0)
			build_info.update_step(name, state="inprogress", log=os.path.basename(LOGFILE), start=utils.get_time(), finish="")

			(code, output) = build_env.ssh.execute("%s cd %s  " % (step_env_pre_cmd, remote_src_dir + package.info['POSTBUILD_STEP%d' % counter] ), env=remote_env, logger=log_obj, output_timeout=timeout)
			state = get_state(code, test=1)
			if code:
				print "Step %d (%s) failed..." % (counter, package.info['POSTBUILD_STEP_NAME%d' % counter])
				build_info.update_step(name, state=state, finish=utils.get_time())
			else:
				build_info.update_step(name, state=state, finish=utils.get_time())
		else:
			break

		counter += 1


state = build_info.get_collective_state()
build_info.update_build(state=state, finish=utils.get_time())

build_env.unlock_env()

print "Done!"

